{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brettzimmerman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing the neccesary packages\n",
    "\n",
    "#Numpy packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#NLTK Packages\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "#Sklearn Packages\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "#Imblearn Packages\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#String Packages\n",
    "import re\n",
    "import string\n",
    "\n",
    "#Word2Vec and simple transformers for RoBERTa\n",
    "from gensim.models import word2vec\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "import pickle\n",
    "\n",
    "#Instantiating NLTK Stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Concatonating the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvey = pd.read_csv('data/harvey.tsv', sep='\\t')\n",
    "irma = pd.read_csv('data/irma.tsv', sep='\\t')\n",
    "matthew = pd.read_csv('data/matthew.tsv', sep='\\t')\n",
    "maria = pd.read_csv('data/maria.tsv', sep='\\t')\n",
    "twint = pd.read_csv('data/twint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6378 entries, 0 to 6377\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   tweet_id     6378 non-null   int64 \n",
      " 1   tweet_text   6378 non-null   object\n",
      " 2   class_label  6378 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 149.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6579 entries, 0 to 6578\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   tweet_id     6579 non-null   int64 \n",
      " 1   tweet_text   6579 non-null   object\n",
      " 2   class_label  6579 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 154.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1157 entries, 0 to 1156\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   tweet_id     1157 non-null   int64 \n",
      " 1   tweet_text   1157 non-null   object\n",
      " 2   class_label  1157 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 27.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5094 entries, 0 to 5093\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   tweet_id     5094 non-null   int64 \n",
      " 1   tweet_text   5094 non-null   object\n",
      " 2   class_label  5094 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 119.5+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32 entries, 0 to 31\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   tweet_text   32 non-null     object\n",
      " 1   class_label  32 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 640.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Checking for Null values\n",
    "canes = [harvey, irma, matthew, maria, twint]\n",
    "for i in canes:\n",
    "    print(i.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe hurricanes, concatonating all the previous dataframes together\n",
    "hurricanes = pd.concat(canes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19240 entries, 0 to 31\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   tweet_id     19208 non-null  float64\n",
      " 1   tweet_text   19240 non-null  object \n",
      " 2   class_label  19240 non-null  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 601.2+ KB\n"
     ]
    }
   ],
   "source": [
    "hurricanes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rescue_volunteering_or_donation_effort    4701\n",
       "other_relevant_information                4214\n",
       "infrastructure_and_utility_damage         3293\n",
       "sympathy_and_support                      1587\n",
       "injured_or_dead_people                    1482\n",
       "displaced_people_and_evacuations          1129\n",
       "caution_and_advice                         987\n",
       "not_humanitarian                           959\n",
       "requests_or_urgent_needs                   856\n",
       "emergency                                   32\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the distribution of Target classes\n",
    "hurricanes['class_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relief = displaced_people_and_evacuations, rescue_volunteering_or_donation_effort, caution_and_advice\n",
    "danger = injured_or_dead_people, infrastructure_and_utility_damage\n",
    "emergencies = requests_or_urgent_needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are too many target classes for my ideal model\n",
    "#I will merge these nine targets down to four\n",
    "hurricanes['class_label'] = hurricanes['class_label'].replace({'displaced_people_and_evacuations': 'relief'})\n",
    "hurricanes['class_label'] = hurricanes['class_label'].replace({'rescue_volunteering_or_donation_effort': 'relief'})\n",
    "\n",
    "hurricanes['class_label'] = hurricanes['class_label'].replace({'caution_and_advice': 'danger'})\n",
    "hurricanes['class_label'] = hurricanes['class_label'].replace({'injured_or_dead_people': 'danger'})\n",
    "hurricanes['class_label'] = hurricanes['class_label'].replace({'infrastructure_and_utility_damage': 'danger'})\n",
    "\n",
    "hurricanes['class_label'] = hurricanes['class_label'].replace({'requests_or_urgent_needs': 'emergency'})\n",
    "\n",
    "hurricanes['class_label'] = hurricanes['class_label'].replace({'other_relevant_information': 'other'})\n",
    "hurricanes['class_label'] = hurricanes['class_label'].replace({'sympathy_and_support': 'other'})\n",
    "hurricanes['class_label'] = hurricanes['class_label'].replace({'not_humanitarian': 'other'})\n",
    "\n",
    "#Exporting csv file to use for visualizations\n",
    "hurricanes.to_csv('hurricanes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other        6760\n",
       "relief       5830\n",
       "danger       5762\n",
       "emergency     888\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Much more manageable\n",
    "hurricanes['class_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.033888e+17</td>\n",
       "      <td>Hurricane Harvey killed at least 38 people, bu...</td>\n",
       "      <td>danger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.011364e+17</td>\n",
       "      <td>Harvey upped to Category 2 hurricane with 110+...</td>\n",
       "      <td>other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.028537e+17</td>\n",
       "      <td>A huge shoutout @TexasGuard for all the work y...</td>\n",
       "      <td>relief</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.028597e+17</td>\n",
       "      <td>Our thoughts and prayers are with the people h...</td>\n",
       "      <td>other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.014060e+17</td>\n",
       "      <td>Homes destroyed on Broadway St in Rockport. Pe...</td>\n",
       "      <td>danger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id                                         tweet_text  \\\n",
       "0  9.033888e+17  Hurricane Harvey killed at least 38 people, bu...   \n",
       "1  9.011364e+17  Harvey upped to Category 2 hurricane with 110+...   \n",
       "2  9.028537e+17  A huge shoutout @TexasGuard for all the work y...   \n",
       "3  9.028597e+17  Our thoughts and prayers are with the people h...   \n",
       "4  9.014060e+17  Homes destroyed on Broadway St in Rockport. Pe...   \n",
       "\n",
       "  class_label  label  \n",
       "0      danger      0  \n",
       "1       other      2  \n",
       "2      relief      3  \n",
       "3       other      2  \n",
       "4      danger      0  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here, I am label encoding the targets from 0-4 so it can be used by various machine learning models.\n",
    "le = LabelEncoder()\n",
    "le.fit(hurricanes['class_label'])\n",
    "hurricanes['label'] = le.transform(hurricanes['class_label'])\n",
    "hurricanes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.351351\n",
       "3    0.303015\n",
       "0    0.299480\n",
       "1    0.046154\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There is an unbalanced distribution of targets. Therefore, when modeling, I will be trying to maximize F1 Score.\n",
    "hurricanes['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to clean the Tweets with Regex\n",
    "def remove_junk(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9#]+', '', str(text)) #remove @mentions \n",
    "    text = re.sub(r'RT[\\s]+', '', str(text)) # remove RT\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) # removes actual links\n",
    "    text = re.sub(r'#', '', str(text)) # remove hashtag symbol\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to tokenize text\n",
    "def tokenize(text):\n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    tokenized_tweet = regex_token.tokenize(text)\n",
    "    tweet_tokens = [word.lower() for word in tokenized_tweet]\n",
    "    return tweet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding punctuation and stopwords to my list so they wont influence the sentiment analysis\n",
    "new_stopwords = ['hurricane', 'harvey', 'irma', 'matthew', 'maria', 'hurricaneharvey', 'hurricanemaria', 'hurricaneirma']\n",
    "punctuations = string.punctuation\n",
    "stopwords.extend(new_stopwords)\n",
    "stopwords.extend(punctuations)\n",
    "\n",
    "#Creating a function that removes the previously defined stopwords\n",
    "def remove_sw_punct(tweet_tokens):\n",
    "    tweets_clean = [word for word in tweet_tokens if word not in stopwords]\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function that stems the tokens down to its root\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming(tweets_clean):\n",
    "    tweets_stem = [stemmer.stem(token) for token in tweets_clean]         \n",
    "    return tweets_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a lemmatization function\n",
    "def pos_tagger(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n",
    "    words_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]   \n",
    "    lemma_list = [wd.lemmatize(tag) for wd, tag in words_tags]\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the preprocessing into a function for lemmatized text\n",
    "def lemma_tweet(text):\n",
    "    processed_tweet = remove_junk(text)\n",
    "    tweet_tokens = pos_tagger(processed_tweet)\n",
    "    tweet_lemma = [word.lower() for word in tweet_tokens]\n",
    "    tweets_lemma = ' '.join(tweet_lemma)\n",
    "    return tweets_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, I am creating three new columns.\n",
    "#model_text is partly processed text that will be fed into the model.\n",
    "#model_text_lemma is also partly processed text, but lemmatization is added.\n",
    "hurricanes['model_text'] = hurricanes['tweet_text'].apply(remove_junk)\n",
    "hurricanes['model_text_lemma'] = hurricanes['tweet_text'].apply(lemma_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class_label</th>\n",
       "      <th>label</th>\n",
       "      <th>model_text</th>\n",
       "      <th>model_text_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.033888e+17</td>\n",
       "      <td>Hurricane Harvey killed at least 38 people, bu...</td>\n",
       "      <td>danger</td>\n",
       "      <td>0</td>\n",
       "      <td>Hurricane Harvey killed at least 38 people, bu...</td>\n",
       "      <td>hurricane harvey kill at least 38 people but c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.011364e+17</td>\n",
       "      <td>Harvey upped to Category 2 hurricane with 110+...</td>\n",
       "      <td>other</td>\n",
       "      <td>2</td>\n",
       "      <td>Harvey upped to Category 2 hurricane with 110+...</td>\n",
       "      <td>harvey up to category 2 hurricane with 110+ mp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.028537e+17</td>\n",
       "      <td>A huge shoutout @TexasGuard for all the work y...</td>\n",
       "      <td>relief</td>\n",
       "      <td>3</td>\n",
       "      <td>A huge shoutout  for all the work your USArmy ...</td>\n",
       "      <td>a huge shoutout for all the work your usarmy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.028597e+17</td>\n",
       "      <td>Our thoughts and prayers are with the people h...</td>\n",
       "      <td>other</td>\n",
       "      <td>2</td>\n",
       "      <td>Our thoughts and prayers are with the people h...</td>\n",
       "      <td>our thought and prayer be with the people hit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.014060e+17</td>\n",
       "      <td>Homes destroyed on Broadway St in Rockport. Pe...</td>\n",
       "      <td>danger</td>\n",
       "      <td>0</td>\n",
       "      <td>Homes destroyed on Broadway St in Rockport. Pe...</td>\n",
       "      <td>homes destroy on broadway st in rockport peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Any LI volunteer FFs who can help Breezy Point...</td>\n",
       "      <td>emergency</td>\n",
       "      <td>1</td>\n",
       "      <td>Any LI volunteer FFs who can help Breezy Point...</td>\n",
       "      <td>any li volunteer ffs who can help breezy point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>@GovChristie We're working on putting an army ...</td>\n",
       "      <td>emergency</td>\n",
       "      <td>1</td>\n",
       "      <td>We're working on putting an army together of ...</td>\n",
       "      <td>we 're work on put an army together of everyda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hoping my Hoboken friend, Maria (macabfilms), ...</td>\n",
       "      <td>emergency</td>\n",
       "      <td>1</td>\n",
       "      <td>Hoping my Hoboken friend, Maria (macabfilms), ...</td>\n",
       "      <td>hoping my hoboken friend maria macabfilms be s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Quick! I need a title for this Hurricane Sandy...</td>\n",
       "      <td>emergency</td>\n",
       "      <td>1</td>\n",
       "      <td>Quick! I need a title for this Hurricane Sandy...</td>\n",
       "      <td>quick i need a title for this hurricane sandy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>@billritter7 please help us lower manhattan se...</td>\n",
       "      <td>emergency</td>\n",
       "      <td>1</td>\n",
       "      <td>please help us lower manhattan seaport totall...</td>\n",
       "      <td>please help u low manhattan seaport totally de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19240 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id                                         tweet_text  \\\n",
       "0   9.033888e+17  Hurricane Harvey killed at least 38 people, bu...   \n",
       "1   9.011364e+17  Harvey upped to Category 2 hurricane with 110+...   \n",
       "2   9.028537e+17  A huge shoutout @TexasGuard for all the work y...   \n",
       "3   9.028597e+17  Our thoughts and prayers are with the people h...   \n",
       "4   9.014060e+17  Homes destroyed on Broadway St in Rockport. Pe...   \n",
       "..           ...                                                ...   \n",
       "27           NaN  Any LI volunteer FFs who can help Breezy Point...   \n",
       "28           NaN  @GovChristie We're working on putting an army ...   \n",
       "29           NaN  Hoping my Hoboken friend, Maria (macabfilms), ...   \n",
       "30           NaN  Quick! I need a title for this Hurricane Sandy...   \n",
       "31           NaN  @billritter7 please help us lower manhattan se...   \n",
       "\n",
       "   class_label  label                                         model_text  \\\n",
       "0       danger      0  Hurricane Harvey killed at least 38 people, bu...   \n",
       "1        other      2  Harvey upped to Category 2 hurricane with 110+...   \n",
       "2       relief      3  A huge shoutout  for all the work your USArmy ...   \n",
       "3        other      2  Our thoughts and prayers are with the people h...   \n",
       "4       danger      0  Homes destroyed on Broadway St in Rockport. Pe...   \n",
       "..         ...    ...                                                ...   \n",
       "27   emergency      1  Any LI volunteer FFs who can help Breezy Point...   \n",
       "28   emergency      1   We're working on putting an army together of ...   \n",
       "29   emergency      1  Hoping my Hoboken friend, Maria (macabfilms), ...   \n",
       "30   emergency      1  Quick! I need a title for this Hurricane Sandy...   \n",
       "31   emergency      1   please help us lower manhattan seaport totall...   \n",
       "\n",
       "                                     model_text_lemma  \n",
       "0   hurricane harvey kill at least 38 people but c...  \n",
       "1   harvey up to category 2 hurricane with 110+ mp...  \n",
       "2   a huge shoutout for all the work your usarmy s...  \n",
       "3   our thought and prayer be with the people hit ...  \n",
       "4   homes destroy on broadway st in rockport peopl...  \n",
       "..                                                ...  \n",
       "27  any li volunteer ffs who can help breezy point...  \n",
       "28  we 're work on put an army together of everyda...  \n",
       "29  hoping my hoboken friend maria macabfilms be s...  \n",
       "30  quick i need a title for this hurricane sandy ...  \n",
       "31  please help u low manhattan seaport totally de...  \n",
       "\n",
       "[19240 rows x 6 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hurricanes dataframe with the three new columns\n",
    "hurricanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing a train test split. The model_text column will be used here.\n",
    "X = hurricanes.drop('label', axis=1)\n",
    "y = hurricanes['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, train_size = .8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.30      0.30      4610\n",
      "           1       0.06      0.06      0.06       710\n",
      "           2       0.35      0.35      0.35      5408\n",
      "           3       0.30      0.30      0.30      4664\n",
      "\n",
      "    accuracy                           0.31     15392\n",
      "   macro avg       0.25      0.25      0.25     15392\n",
      "weighted avg       0.31      0.31      0.31     15392\n",
      "\n",
      "\n",
      "\t\tThe Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.31      0.31      1152\n",
      "           1       0.06      0.06      0.06       178\n",
      "           2       0.36      0.35      0.36      1352\n",
      "           3       0.31      0.32      0.32      1166\n",
      "\n",
      "    accuracy                           0.32      3848\n",
      "   macro avg       0.26      0.26      0.26      3848\n",
      "weighted avg       0.32      0.32      0.32      3848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dum_pipe = Pipeline([('count', CountVectorizer()),\n",
    "                    ('model', DummyClassifier(strategy='stratified'))])\n",
    "\n",
    "model = dum_pipe.fit(X_train['model_text'], y_train)\n",
    "\n",
    "y_trn_pred = dum_pipe.predict(X_train['model_text'])\n",
    "y_tst_pred = dum_pipe.predict(X_test['model_text'])\n",
    "    \n",
    "print('\\t\\tThe Train Results')\n",
    "print(classification_report(y_train, y_trn_pred))\n",
    "print('\\n\\t\\tThe Test Results')\n",
    "print(classification_report(y_test, y_tst_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brettzimmerman/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      4610\n",
      "           1       0.65      0.93      0.77       710\n",
      "           2       0.88      0.85      0.87      5408\n",
      "           3       0.91      0.90      0.91      4664\n",
      "\n",
      "    accuracy                           0.89     15392\n",
      "   macro avg       0.84      0.90      0.86     15392\n",
      "weighted avg       0.89      0.89      0.89     15392\n",
      "\n",
      "\n",
      "\t\tThe Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      1152\n",
      "           1       0.49      0.71      0.58       178\n",
      "           2       0.75      0.74      0.74      1352\n",
      "           3       0.82      0.79      0.80      1166\n",
      "\n",
      "    accuracy                           0.79      3848\n",
      "   macro avg       0.73      0.77      0.75      3848\n",
      "weighted avg       0.79      0.79      0.79      3848\n",
      "\n",
      "Test Score:  0.7853430353430353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brettzimmerman/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/brettzimmerman/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/brettzimmerman/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/brettzimmerman/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:  0.779755774988335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brettzimmerman/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords, tokenizer=tokenize)),\n",
    "    ('smt', SMOTE(random_state=30)),\n",
    "    ('lr', LogisticRegression()),\n",
    "])\n",
    "\n",
    "lr_model = lr_pipe.fit(X_train['model_text'], y_train)\n",
    "    \n",
    "y_trn_pred = lr_model.predict(X_train['model_text'])\n",
    "y_tst_pred = lr_model.predict(X_test['model_text'])\n",
    "    \n",
    "print('\\t\\tThe Train Results')\n",
    "print(classification_report(y_train, y_trn_pred))\n",
    "print('\\n\\t\\tThe Test Results')\n",
    "print(classification_report(y_test, y_tst_pred))\n",
    "print('Test Score: ', lr_model.score(X_test['model_text'], y_test))\n",
    "cv_score = cross_val_score(lr_model, X_train['model_text'], y_train)\n",
    "print('Cross Validation Score: ', cv_score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch on a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1728 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=2)]: Done 124 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=2)]: Done 284 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=2)]: Done 508 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=2)]: Done 1148 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=2)]: Done 1564 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=2)]: Done 2044 tasks      | elapsed: 22.0min\n",
      "[Parallel(n_jobs=2)]: Done 2588 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=2)]: Done 3196 tasks      | elapsed: 36.7min\n",
      "[Parallel(n_jobs=2)]: Done 3456 out of 3456 | elapsed: 40.5min finished\n",
      "/Users/brettzimmerman/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      4610\n",
      "           1       0.71      0.97      0.82       710\n",
      "           2       0.91      0.86      0.88      5408\n",
      "           3       0.92      0.92      0.92      4664\n",
      "\n",
      "    accuracy                           0.90     15392\n",
      "   macro avg       0.87      0.92      0.89     15392\n",
      "weighted avg       0.91      0.90      0.90     15392\n",
      "\n",
      "\n",
      "\t\tThe Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86      1152\n",
      "           1       0.52      0.69      0.59       178\n",
      "           2       0.75      0.75      0.75      1352\n",
      "           3       0.83      0.79      0.81      1166\n",
      "\n",
      "    accuracy                           0.79      3848\n",
      "   macro avg       0.74      0.77      0.75      3848\n",
      "weighted avg       0.80      0.79      0.79      3848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_lr_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords, tokenizer=tokenize)),\n",
    "    ('smt', SMOTE(random_state=30)),\n",
    "    ('lr', LogisticRegression()),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.75),\n",
    "    'tfidf__min_df': (2, 3),\n",
    "    'tfidf__ngram_range': [(1, 1),(1, 2)],\n",
    "    'tfidf__max_features': (8000, 10000, 70000),\n",
    "    'lr__C': (.7, 1, 1.5),\n",
    "    'lr__class_weight' : (['balanced']),\n",
    "    'lr__solver': ('newton-cg', 'sag', 'saga', 'lbfgs'),\n",
    "    'lr__max_iter': (100, 200, 300),\n",
    "    'smt__k_neighbors' : (2, 5)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(tf_lr_pipe, parameters, cv=2, n_jobs=2, verbose=3, scoring = 'f1_macro')\n",
    "grid_search.fit(X_train['model_text'], y_train)\n",
    "\n",
    "tf_lr_best = grid_search.best_estimator_\n",
    "\n",
    "y_trn_pred = tf_lr_best.predict(X_train['model_text'])\n",
    "y_tst_pred = tf_lr_best.predict(X_test['model_text'])\n",
    "\n",
    "print('\\t\\tThe Train Results')\n",
    "print(classification_report(y_train, y_trn_pred))\n",
    "print('\\n\\t\\tThe Test Results')\n",
    "print(classification_report(y_test, y_tst_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the best parameters according to the gridsearch\n",
    "with open('logistic_regression.pickle', 'wb') as f:\n",
    "    pickle.dump(tf_lr_best, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search on MultinomialNB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=2)]: Done 124 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=2)]: Done 284 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=2)]: Done 288 out of 288 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90      4610\n",
      "           1       0.52      0.98      0.68       710\n",
      "           2       0.92      0.75      0.83      5408\n",
      "           3       0.88      0.89      0.89      4664\n",
      "\n",
      "    accuracy                           0.86     15392\n",
      "   macro avg       0.80      0.89      0.82     15392\n",
      "weighted avg       0.88      0.86      0.86     15392\n",
      "\n",
      "\n",
      "\t\tThe Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80      1152\n",
      "           1       0.37      0.77      0.50       178\n",
      "           2       0.73      0.56      0.63      1352\n",
      "           3       0.76      0.77      0.77      1166\n",
      "\n",
      "    accuracy                           0.72      3848\n",
      "   macro avg       0.66      0.73      0.68      3848\n",
      "weighted avg       0.73      0.72      0.72      3848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_nb_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords, tokenizer=tokenize)),\n",
    "    ('smt', SMOTE(random_state=30)),\n",
    "    ('mnb', MultinomialNB()),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.75),\n",
    "    'tfidf__min_df': (2, 3),\n",
    "    'tfidf__ngram_range': [(1, 1),(1, 2)],\n",
    "    'tfidf__max_features': (8000, 10000, 70000),\n",
    "    'mnb__alpha': (.05, .5, 2),\n",
    "    'smt__k_neighbors' : (2, 5)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(tf_nb_pipe, parameters, cv=2, n_jobs=2, verbose=3, scoring = 'f1_macro')\n",
    "grid_search.fit(X_train['model_text'], y_train)\n",
    "\n",
    "tf_nb_best = grid_search.best_estimator_\n",
    "\n",
    "y_trn_pred = tf_nb_best.predict(X_train['model_text'])\n",
    "y_tst_pred = tf_nb_best.predict(X_test['model_text'])\n",
    "    \n",
    "print('\\t\\tThe Train Results')\n",
    "print(classification_report(y_train, y_trn_pred))\n",
    "print('\\n\\t\\tThe Test Results')\n",
    "print(classification_report(y_test, y_tst_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying the best nb parameters on a model using text cleaned with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe Train Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90      4610\n",
      "           1       0.52      0.98      0.68       710\n",
      "           2       0.92      0.75      0.82      5408\n",
      "           3       0.87      0.89      0.88      4664\n",
      "\n",
      "    accuracy                           0.85     15392\n",
      "   macro avg       0.80      0.89      0.82     15392\n",
      "weighted avg       0.87      0.85      0.86     15392\n",
      "\n",
      "\n",
      "\t\tThe Test Results\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.81      1152\n",
      "           1       0.36      0.75      0.49       178\n",
      "           2       0.74      0.57      0.64      1352\n",
      "           3       0.76      0.77      0.77      1166\n",
      "\n",
      "    accuracy                           0.72      3848\n",
      "   macro avg       0.66      0.73      0.68      3848\n",
      "weighted avg       0.74      0.72      0.72      3848\n",
      "\n",
      "Test Score:  0.7216735966735967\n",
      "Cross Validation Score:  0.7160865879468981\n"
     ]
    }
   ],
   "source": [
    "lemma_model = tf_nb_best.fit(X_train['model_text_lemma'], y_train)\n",
    "    \n",
    "y_trn_pred = lemma_model.predict(X_train['model_text_lemma'])\n",
    "y_tst_pred = lemma_model.predict(X_test['model_text_lemma'])\n",
    "    \n",
    "print('\\t\\tThe Train Results')\n",
    "print(classification_report(y_train, y_trn_pred))\n",
    "print('\\n\\t\\tThe Test Results')\n",
    "print(classification_report(y_test, y_tst_pred))\n",
    "print('Test Score: ', tf_nb_best.score(X_test['model_text_lemma'], y_test))\n",
    "cv_score = cross_val_score(tf_nb_best, X_train['model_text_lemma'], y_train)\n",
    "print('Cross Validation Score: ', cv_score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The Scores are worse, but the overfitting is reduced so I will save this as the naive bayes model.\n",
    "with open('naive_bayes.pickle', 'wb') as f:\n",
    "    pickle.dump(lemma_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Into More Advanced Models: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the correct text and target labels that Word2Vec is expecting\n",
    "np.random.seed(0)\n",
    "target = hurricanes['class_label']\n",
    "data = hurricanes['model_text'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28534 unique tokens in the dataset.\n"
     ]
    }
   ],
   "source": [
    "#Taking a look at the unique tokens\n",
    "total_vocabulary = set(word for tweet in data for word in tweet)\n",
    "len(total_vocabulary)\n",
    "print('There are {} unique tokens in the dataset.'.format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the glove corpus which Word2Vec is trained on\n",
    "glove = {}\n",
    "with open('glove.6B/glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning Word2Vec into a class so that it can be used in a pipeline with models\n",
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating pipelines using Word2Vec and differnt ml models\n",
    "rf = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "               ('smt', SMOTE(random_state=30)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('smt', SMOTE(random_state=30)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "               ('smt', SMOTE(random_state=30)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "/Users/brettzimmerman/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/brettzimmerman/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.5876819126819126),\n",
       " ('Support Vector Machine', 0.5744282744282745),\n",
       " ('Logistic Regression', 0.5581081081081081)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not very impressive. Further tuning may be required\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a ClassificationModel using roberta\n",
    "roberta = ClassificationModel('roberta', 'roberta-base', num_labels=4, use_cuda=False, weight=[1, 3, 1, 1],\n",
    "                              args={'learning_rate':1e-5, 'num_train_epochs': 2, 'overwrite_output_dir': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing the data to a format the model will accept\n",
    "transformer_df = hurricanes[['tweet_text', 'label']]\n",
    "transformer_df = transformer_df.rename(columns={\"tweet_text\" : \"text\"})\n",
    "transformer_df = transformer_df.rename(columns={\"label\" : \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split on teh transformer ready data\n",
    "train_sample, test_sample = train_test_split(transformer_df, random_state=42, train_size = .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa47d20f2f148fdbee92702ba67996a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15392.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d43c55c816344199e3d31840708faaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1bd722bdfb4fe1828fbb94c2091431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 0 of 2'), FloatProgress(value=0.0, max=1924.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f83e86be304a4dbafd01bbdd2f481b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 1 of 2'), FloatProgress(value=0.0, max=1924.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3848, 0.5614642921792588)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training roberta on the sample data\n",
    "roberta.train_model(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5d2d00b9b64b8aa86b5fe2bffe62dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3848.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c706d527d86d4a5cbfa1bc234896cae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Evaluation'), FloatProgress(value=0.0, max=481.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model on the test data\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "    \n",
    "result, model_outputs, wrong_predictions = roberta.eval_model(test_sample, f1=f1_multiclass, acc=accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.7442880770167848,\n",
       " 'f1': 0.8196465696465697,\n",
       " 'acc': 0.8196465696465697,\n",
       " 'eval_loss': 0.5939578148797774}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Impressive metrics\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Scores are worse, but the overfitting is reduced so I will save this as the naive bayes model.\n",
    "with open('roberta.pickle', 'wb') as f:\n",
    "    pickle.dump(roberta, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "danger    = 0\n",
    "emergency = 1\n",
    "other     = 2\n",
    "relief    = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
